{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7ttBYeclcLI"
      },
      "source": [
        "# Is Synthetic Data Real?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQGm6JtgehIq"
      },
      "source": [
        "> Harvard CS 242: Computing at Scale (Fall 2023)\n",
        "\n",
        ">\n",
        "> Instructor: Professor HT Kung\n",
        "\n",
        "> Authors: Michael Xiang, Marcos Johnson-Noya, Minkai Li, Corwin Cheung\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UvFA89jTuON"
      },
      "source": [
        "---\n",
        "\n",
        "### **1. Using CLIP as an indicator of text specificity.**\n",
        "\n",
        "---\n",
        "We will be using the CIFAR-10 dataset (CIFAR-10). We will run the dataset through CLIP and test if classes that performed better on CLIP also perform better for StableRep.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hJBdvnBy_qt"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import sys\n",
        "from collections import OrderedDict\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9u_KzFV0sLA"
      },
      "source": [
        "**Downloading CLIP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEcpVq7C0y1R"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2STZ28qVhMqR",
        "outputId": "ec482d22-667e-4fc0-e0de-2f3dcee52d54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:03<00:00, 110MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# Importing clip and the model\n",
        "import clip\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EYLHx1Z1pCc"
      },
      "source": [
        "**Loading in different datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9WL6HA_Lpe8",
        "outputId": "ac54034e-0a11-4996-86b5-2465a647496a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load the CIFAR-10 test dataset with the new transformations\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=preprocess)\n",
        "\n",
        "# DataLoader\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "outJt5VLrmCq",
        "outputId": "39653ec6-9727-49db-8fd4-6116c226cb32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz to data/oxford-iiit-pet/images.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 791918971/791918971 [00:30<00:00, 25561340.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/oxford-iiit-pet/images.tar.gz to data/oxford-iiit-pet\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz to data/oxford-iiit-pet/annotations.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19173078/19173078 [00:01<00:00, 11541017.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/oxford-iiit-pet/annotations.tar.gz to data/oxford-iiit-pet\n"
          ]
        }
      ],
      "source": [
        "# Load the OxfordIIITPet test dataset with the new transformations\n",
        "testset = torchvision.datasets.OxfordIIITPet(root='./data', split=\"test\",\n",
        "                                       download=True, transform=preprocess)\n",
        "\n",
        "# DataLoadert\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9CyAZbbxGln",
        "outputId": "75dc621e-c53d-4cf9-cb5e-be77b73d40a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./data/stl10_binary.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2640397119/2640397119 [03:00<00:00, 14654695.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/stl10_binary.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "# Load the STL10 test dataset with the new transformations\n",
        "testset = torchvision.datasets.STL10(root='./data', split=\"test\",\n",
        "                                       download=True, transform=preprocess)\n",
        "\n",
        "# DataLoadert\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm2mPzjwQ5pk"
      },
      "outputs": [],
      "source": [
        "# Load the LSUN test dataset with the new transformations\n",
        "testset = torchvision.datasets.FashionMNIST(\"./data\", train = False, transform = preprocess, download = True)\n",
        "\n",
        "# DataLoader\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0Gn4k58MO6f",
        "outputId": "0704b70a-e2e8-488e-e71c-82be57ca3dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqn2sIG0NQ2-",
        "outputId": "430de3c0-6ae2-4c60-9963-8a0c21ac5c05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "cd '/content/drive/MyDrive/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3mZckGFPWEx",
        "outputId": "bb8191c3-8acf-46a1-ec13-c525224bf8f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;36m'242 stuff'\u001b[0m@                          \u001b[01;34m'Generated Images For StableRep Testing'\u001b[0m/   \u001b[01;34mtest\u001b[0m/\n",
            "'Alien Music Theory.gslides'           IMG_1226.JPG                               \u001b[01;34mvalid\u001b[0m/\n",
            "\u001b[01;34m'Colab Notebooks'\u001b[0m/                    'MHD PINN Diagonals.ipynb'\n",
            "'Copy of 2023-CS124-lec22-notes.pdf'  'Michael Xiang - Resume.pdf'\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcRH1rutLTS8"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "test_dataset = ImageFolder(\"valid/\",\n",
        "                      transform = preprocess)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIgoGTZgwro0"
      },
      "source": [
        "**Running Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "moBnAk4sOHpu"
      },
      "outputs": [],
      "source": [
        "def get_text_prompts(classes):\n",
        "    result = []\n",
        "    for c in classes:\n",
        "        result.append(f\"A photo of a {c}\")\n",
        "    return result\n",
        "\n",
        "# classes = [\"Apple Pie\", \"Bibimbap\", \"Cannoli\", \"Edamame\", \"Falafel\", \"French Toast\", \"Ice Cream\", \"Ramen\", \"Sushi\", \"Tiramisu\"]\n",
        "# classes = ['Abyssinian', 'Bengal', 'Birman', 'Bombay', 'British Shorthair', 'Egyptian Mau', 'Maine Coon', 'Persian', 'Ragdoll', 'Russian Blue', 'Siamese', 'Sphynx', 'american bulldog', 'american pit bull terrier', 'basset hound', 'beagle', 'boxer', 'chihuahua', 'english cocker spaniel', 'english setter', 'german shorthaired', 'great pyrenees', 'havanese', 'japanese chin', 'keeshond', 'leonberger', 'miniature pinscher', 'newfoundland', 'pomeranian', 'pug', 'saint bernard', 'samoyed', 'scottish terrier', 'shiba inu', 'staffordshire bull terrier', 'wheaten terrier', 'yorkshire terrier']\n",
        "classes = [\"airplane\", \"bird\", \"car\", \"cat\", \"deer\", \"dog\", \"horse\", \"monkey\", \"ship\", \"truck\"]\n",
        "# classes = [\"agricultural\",\"airplane\",\"baseball diamond\",\"beach\",\n",
        "#  \"buildings\",\"chaparral\",\"dense residential\",\"forest\",\"freeway\",\"golf course\",\"harbor\",\n",
        "#  \"intersection\",\"medium residential\",\"mobile homepark\",\"overpass\",\n",
        "#  \"parking lot\",\"river\",\"runway\",\"sparse residential\",\"storage tanks\",\"tennis court\"]\n",
        "# classes = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n",
        "\n",
        "\n",
        "prompts = get_text_prompts(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yzw7OWkA094K"
      },
      "outputs": [],
      "source": [
        "# Tokenize the classes of the CIFAR-10 Dataset\n",
        "text = clip.tokenize(prompts).to(device)\n",
        "\n",
        "correct_label_accuracy = [[0, 0] for _ in range(len(prompts))]\n",
        "\n",
        "# Getting pure accuracy\n",
        "total_images = 0\n",
        "total_correct = 0\n",
        "\n",
        "# Iterate over the DataLoader\n",
        "for batch in testloader:\n",
        "    images, labels = batch\n",
        "\n",
        "    # Process the batch of images\n",
        "    images = torch.stack([image for image in images]).to(device)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(images)\n",
        "        text_features = model.encode_text(text)\n",
        "\n",
        "        # Calculate logits\n",
        "        logits_per_image, logits_per_text = model(images, text)\n",
        "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "        for i, prob in enumerate(probs):\n",
        "            total_images += 1\n",
        "            correct_label_accuracy[labels[i]][1] += 1\n",
        "            if np.argmax(prob) == labels[i]:\n",
        "                correct_label_accuracy[labels[i]][0] += 1\n",
        "                total_correct += 1\n",
        "print(total_images)\n",
        "print(total_correct)\n",
        "print(total_correct/total_images)\n",
        "for i, (num_correct, num_total) in enumerate(correct_label_accuracy):\n",
        "    print(f\"Class {i} Photos: {num_total}; Accuracy: {num_correct/num_total}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhbpZEQjwlp_"
      },
      "source": [
        "**Plotting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lu-T4wTkgrc"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data for plotting\n",
        "classes = range(len(prompts))\n",
        "class_accuracies = [correct/total for correct, total in correct_label_accuracy]\n",
        "total_accuracy = total_correct/total_images\n",
        "\n",
        "# Plotting the accuracies\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(classes, class_accuracies, color=\"blue\", label=\"Per Class Accuracy\")\n",
        "plt.axhline(y=total_accuracy, color=\"red\", linestyle=\"-\", label=f\"Total Accuracy - {total_accuracy}\")\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy per Class and Total Accuracy\")\n",
        "plt.xticks(classes)\n",
        "plt.yticks([i * 0.1 for i in range(11)])\n",
        "plt.legend()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}